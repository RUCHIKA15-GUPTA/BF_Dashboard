{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c53ed-06fc-4e5c-a5b7-7566324f28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pmdarima.arima import auto_arima\n",
    "import pmdarima as pmd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mysql.connector\n",
    "import findspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "from pyspark.sql.functions import col, lit, udf,trunc,concat_ws\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML, display\n",
    "from builtins import max\n",
    "from builtins import min\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecd1baa-c8b3-454e-8d34-e266f4c8f7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/VECVNET/zzasom/jupyter/01_Developement_work/01_Codes/Black Forecasting/Monte Carlo/Dec/1153/Builds'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc8ac93-439e-4df0-8543-4b562ad875ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the path file which contains the path for credential json file\n",
    "filepath = open(\"/home/VECVNET/CONFIG/path.json\", \"r\")\n",
    "path = json.load(filepath)\n",
    "filepath.close()\n",
    "path = list(path.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d256a33-1cb8-4d44-9c3b-c652b3a17cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r enteredCred\n",
    "user=enteredCred[0]\n",
    "password=enteredCred[1]\n",
    "db=enteredCred[2]\n",
    "url=enteredCred[3]\n",
    "driver=enteredCred[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d741f4-5efb-4efa-aae8-3dd5c7ab1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/config.json') as f:\n",
    "    data = json.load(f)\n",
    "    growthRate = data['growthRate']\n",
    "    cores_max = data['cores_max']\n",
    "    executor_memory = data['executor_memory']\n",
    "    driver_memory = data['driver_memory']\n",
    "    debug_maxToStringFields = data['debug_maxToStringFields']\n",
    "    default_parallelism = data['default_parallelism']\n",
    "    driver_maxResultSize = data['driver_maxResultSize']\n",
    "    spark_network_timeout = data['spark_network_timeout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624b8926-1dc3-429f-a456-111e7cd7a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b53c8b5-7648-4c0b-9d8f-d4d0389d0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/30 10:51:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# setting up the different config required in the server\n",
    "# This configs help to run the spark session uninteruptedlly\n",
    "# The config in the spark session can be readilly updated to improve the running process of the code\n",
    "# The session would bind a port in spark UI and the progress and also be monitored.\n",
    "# set warning as false to remove the messages\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://10.106.111.117:7077\") \\\n",
    "        .appName(\"Monte Carlo - purchasing group buyer counts\") \\\n",
    "        .config(\"spark.cores.max\",2) \\\n",
    "        .config(\"spark.driver.memory\",driver_memory) \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config(\"spark.debug.maxToStringFields\",debug_maxToStringFields) \\\n",
    "        .config(\"spark.default.parallelism\",default_parallelism) \\\n",
    "        .config(\"spark.driver.maxResultSize\",driver_maxResultSize) \\\n",
    "        .config(\"spark.network.timeout\",spark_network_timeout) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ff5af9-a9ac-4eb6-969d-79bcc19619f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing arrow from spark for faster execution\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82f17c-8905-407d-be47-9a345f3c4629",
   "metadata": {},
   "source": [
    "#### code for loop creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dbdc8ba-4755-4e32-bac0-7867cef4b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Purchasing Group Details  Buyer_counts_26May  Buyer_counts_27May  \\\n",
      "0               Akshay Raj                  35                  27   \n",
      "1           Amarjeet Yadav                  37                  28   \n",
      "\n",
      "   Buyer_counts_28May  Buyer_counts_29May pending % Efficiency %  \n",
      "0                  20                  12    34.29%       65.71%  \n",
      "1                  15                  13    35.14%       64.86%  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "# === Constants ===\n",
    "NUM_SIMULATIONS = 10000\n",
    "INPUT_FOLDER = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Input\"\n",
    "OUTPUT_FOLDER = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "BASE_DATE = datetime(2025, 5, 26)  # Fixed base date: 12 May 2025\n",
    "DAYS_AHEAD = 3  # Today + next 3 days\n",
    "\n",
    "# === Load PG Mapping File ===\n",
    "pg_file_path = os.path.join(INPUT_FOLDER, \"MRPcontrollerPurchasingGrp.csv\")\n",
    "if not os.path.exists(pg_file_path):\n",
    "    raise FileNotFoundError(\"PG file not found.\")\n",
    "\n",
    "df_pg = pd.read_csv(pg_file_path)\n",
    "df_pg['Material_PG_Concat'] = df_pg['Material'].astype(str) + \"_\" + df_pg['Purchasing Group Details'].astype(str)\n",
    "df_pg_merge = df_pg[['Material', 'Purchasing Group Details', 'MRP controller details']].rename(\n",
    "    columns={'Material': 'Material Code'}\n",
    ")\n",
    "\n",
    "# === Function to Process a Single Date ===\n",
    "def process_day(date_obj, filter_from_df):\n",
    "    date_str = date_obj.strftime('%Y%m%d')\n",
    "    file_name = f\"MC_day_wise_{NUM_SIMULATIONS}_{date_str}.xlsx\"\n",
    "    file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[{date_str}] File not found: {file_name}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df_mc = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            if not df_mc.empty:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"[{date_str}] All sheets are empty.\")\n",
    "            return None, None\n",
    "\n",
    "        # Merge PG info\n",
    "        df_mc = pd.merge(df_mc, df_pg_merge, on='Material Code', how='inner')\n",
    "        df_mc['Material_PG_Concat'] = df_mc['Material Code'].astype(str) + \"_\" + df_mc['Purchasing Group Details'].astype(str)\n",
    "        df_mc['Stockout Probability (%)'] = pd.to_numeric(df_mc['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "        # Filter\n",
    "        high_risk_df = df_mc[(df_mc['Stockout Probability (%)'] >= 50) & (df_mc['GIT'] == 0)]\n",
    "        filtered_df = high_risk_df[['Running Date', 'Material Code', 'GIT', 'Purchasing Group Details', 'Material_PG_Concat']]\n",
    "\n",
    "        if filter_from_df is not None:\n",
    "            filtered_df = filtered_df[filtered_df['Material_PG_Concat'].isin(filter_from_df['Material_PG_Concat'])]\n",
    "\n",
    "        # Count high-risk parts per PG\n",
    "        col_label = f\"Buyer_counts_{date_obj.strftime('%d%b')}\"\n",
    "        pg_counts = (\n",
    "            filtered_df.groupby('Purchasing Group Details')\n",
    "            .size()\n",
    "            .reset_index(name=col_label)\n",
    "        )\n",
    "\n",
    "        return filtered_df, pg_counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{date_str}] Error processing: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# === Process Multiple Days ===\n",
    "filtered_df_acc = None\n",
    "pg_counts_all = []\n",
    "\n",
    "for i in range(DAYS_AHEAD + 1):  # today to today + 3\n",
    "    run_date = BASE_DATE + timedelta(days=i)\n",
    "    filtered_df_acc, pg_count = process_day(run_date, filtered_df_acc)\n",
    "    if pg_count is not None:\n",
    "        pg_counts_all.append(pg_count)\n",
    "\n",
    "# === Merge and Compute Efficiency ===\n",
    "if pg_counts_all:\n",
    "    merged_pg_counts = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on='Purchasing Group Details', how='outer'),\n",
    "        pg_counts_all\n",
    "    )\n",
    "\n",
    "    # Fill NaN with 0 and convert to int\n",
    "    count_cols = [col for col in merged_pg_counts.columns if col.startswith(\"Buyer_counts_\")]\n",
    "    merged_pg_counts[count_cols] = merged_pg_counts[count_cols].fillna(0).astype(int)\n",
    "\n",
    "    if len(count_cols) >= 2:\n",
    "        first_col, last_col = count_cols[0], count_cols[-1]\n",
    "        merged_pg_counts['pending %'] = (merged_pg_counts[last_col] / merged_pg_counts[first_col].replace(0, 1)) * 100\n",
    "        merged_pg_counts['Efficiency %'] = 100 - merged_pg_counts['pending %']\n",
    "\n",
    "        merged_pg_counts['pending %'] = merged_pg_counts['pending %'].round(2).astype(str) + '%'\n",
    "        merged_pg_counts['Efficiency %'] = merged_pg_counts['Efficiency %'].round(2).astype(str) + '%'\n",
    "\n",
    "    print(merged_pg_counts.head(2))\n",
    "else:\n",
    "    print(\"No data processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63df0587-2cc7-45a7-812b-9956108a3027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Purchasing Group Details</th>\n",
       "      <th>Buyer_counts_26May</th>\n",
       "      <th>Buyer_counts_27May</th>\n",
       "      <th>Buyer_counts_28May</th>\n",
       "      <th>Buyer_counts_29May</th>\n",
       "      <th>pending %</th>\n",
       "      <th>Efficiency %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akshay Raj</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>34.29%</td>\n",
       "      <td>65.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amarjeet Yadav</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>35.14%</td>\n",
       "      <td>64.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arun Kumar Shah</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>28.95%</td>\n",
       "      <td>71.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashish Purohit</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>58.33%</td>\n",
       "      <td>41.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deepak Deshpande</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>81.25%</td>\n",
       "      <td>18.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deepak Raipure</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>27.08%</td>\n",
       "      <td>72.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Girish R Sonawane</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>30.77%</td>\n",
       "      <td>69.23%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Purchasing Group Details  Buyer_counts_26May  Buyer_counts_27May  \\\n",
       "0               Akshay Raj                  35                  27   \n",
       "1           Amarjeet Yadav                  37                  28   \n",
       "2          Arun Kumar Shah                  38                  24   \n",
       "3           Ashish Purohit                  12                   9   \n",
       "4         Deepak Deshpande                  32                  29   \n",
       "5           Deepak Raipure                  48                  32   \n",
       "6        Girish R Sonawane                  52                  31   \n",
       "\n",
       "   Buyer_counts_28May  Buyer_counts_29May pending % Efficiency %  \n",
       "0                  20                  12    34.29%       65.71%  \n",
       "1                  15                  13    35.14%       64.86%  \n",
       "2                  15                  11    28.95%       71.05%  \n",
       "3                   7                   7    58.33%       41.67%  \n",
       "4                  28                  26    81.25%       18.75%  \n",
       "5                  21                  13    27.08%       72.92%  \n",
       "6                  21                  16    30.77%       69.23%  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_pg_counts.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5da779b-420d-47f2-b461-64f399af7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pg_counts.to_excel('BF_26May_Dashboard_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f59fbd0-f567-4222-96fb-ea55228d6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(merged_pg_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da57627-4ca9-4379-9844-d56eb4f932ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write PySpark DataFrame to the database\n",
    "spark_df.write.mode(\"overwrite\").format(\"jdbc\").options(\n",
    "    driver=driver,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    dbtable=\"Black_Forecasting_Dashboard\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final output store in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c868d2-8a32-4cf2-9ef8-ac0dfc5acfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06391301-25bc-4305-b2c1-16fb9aa35203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
