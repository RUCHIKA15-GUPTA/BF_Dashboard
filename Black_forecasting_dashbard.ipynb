{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453c53ed-06fc-4e5c-a5b7-7566324f28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pmdarima.arima import auto_arima\n",
    "import pmdarima as pmd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mysql.connector\n",
    "import findspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "from pyspark.sql.functions import col, lit, udf,trunc,concat_ws\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML, display\n",
    "from builtins import max\n",
    "from builtins import min\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pmdarima.arima import auto_arima\n",
    "import pmdarima as pmd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mysql.connector\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "from pyspark.sql.functions import col, lit, udf,trunc,concat_ws\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML, display\n",
    "from builtins import max\n",
    "from builtins import min\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # Suppress the warning\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML, display\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecd1baa-c8b3-454e-8d34-e266f4c8f7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/VECVNET/zzasom/jupyter/01_Developement_work/01_Codes/Black Forecasting/Monte Carlo/Dec/1153/Builds'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc8ac93-439e-4df0-8543-4b562ad875ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the path file which contains the path for credential json file\n",
    "filepath = open(\"/home/VECVNET/CONFIG/path.json\", \"r\")\n",
    "path = json.load(filepath)\n",
    "filepath.close()\n",
    "path = list(path.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d256a33-1cb8-4d44-9c3b-c652b3a17cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r enteredCred\n",
    "user=enteredCred[0]\n",
    "password=enteredCred[1]\n",
    "db=enteredCred[2]\n",
    "url=enteredCred[3]\n",
    "driver=enteredCred[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d741f4-5efb-4efa-aae8-3dd5c7ab1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/config.json') as f:\n",
    "    data = json.load(f)\n",
    "    growthRate = data['growthRate']\n",
    "    cores_max = data['cores_max']\n",
    "    executor_memory = data['executor_memory']\n",
    "    driver_memory = data['driver_memory']\n",
    "    debug_maxToStringFields = data['debug_maxToStringFields']\n",
    "    default_parallelism = data['default_parallelism']\n",
    "    driver_maxResultSize = data['driver_maxResultSize']\n",
    "    spark_network_timeout = data['spark_network_timeout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624b8926-1dc3-429f-a456-111e7cd7a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b53c8b5-7648-4c0b-9d8f-d4d0389d0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/30 10:51:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# setting up the different config required in the server\n",
    "# This configs help to run the spark session uninteruptedlly\n",
    "# The config in the spark session can be readilly updated to improve the running process of the code\n",
    "# The session would bind a port in spark UI and the progress and also be monitored.\n",
    "# set warning as false to remove the messages\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://10.106.111.117:7077\") \\\n",
    "        .appName(\"Monte Carlo - purchasing group buyer counts\") \\\n",
    "        .config(\"spark.cores.max\",2) \\\n",
    "        .config(\"spark.driver.memory\",driver_memory) \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config(\"spark.debug.maxToStringFields\",debug_maxToStringFields) \\\n",
    "        .config(\"spark.default.parallelism\",default_parallelism) \\\n",
    "        .config(\"spark.driver.maxResultSize\",driver_maxResultSize) \\\n",
    "        .config(\"spark.network.timeout\",spark_network_timeout) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ff5af9-a9ac-4eb6-969d-79bcc19619f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing arrow from spark for faster execution\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877c222-694f-4e92-ad0f-21667220028d",
   "metadata": {},
   "source": [
    "#PURCHASUNG GROUP FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41d333-bfee-46c2-a4ea-c745d41cf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the PG file\n",
    "\n",
    "#/home/VECVNET/zzasom/jupyter/01_Developement_work/01_Codes/Black Forecasting/Monte Carlo/Dec/1153/Input\n",
    "\n",
    "folder_path1 = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Input\"\n",
    "\n",
    "# Get yesterday's date in YYYYMMDD or any required format\n",
    "#yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "# Build the expected filename\n",
    "filename1 = f\"MRPcontrollerPurchasingGrp.csv\"  # Adjust pattern as per your naming scheme\n",
    "\n",
    "# Full path to the file\n",
    "file_path1 = os.path.join(folder_path1, filename1)\n",
    "\n",
    "# Read the file if it exists\n",
    "if os.path.exists(file_path1):\n",
    "    df_PG = pd.read_csv(file_path1)\n",
    "    print(df_PG.head(2))\n",
    "else:\n",
    "    print(f\"No file found\")\n",
    "\n",
    "#df_PG['Material_PG_Concat'] = df_PG['Material'].astype(str) + \"_\" + df_PG['Purchasing Group Details'].astype(str)\n",
    "df_PG.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e265fe5-c1b8-4362-8563-6e3beea9e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_PG['Material_PG_Concat'] = df_PG['Material'].astype(str) + \"_\" + df_PG['Purchasing Group Details'].astype(str)\n",
    "df_gp = df_PG[['Material', 'Purchasing Group Details']]\n",
    "df_gp['Material_PG_Concat'] = df_gp['Material'].astype(str) + \"_\" + df_gp['Purchasing Group Details'].astype(str)\n",
    "print(df_gp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c245972-2942-4d02-9902-af8287a2d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gp.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b336c-26e5-4891-bc43-348381377a3b",
   "metadata": {},
   "source": [
    "#week - 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67e7e3-4520-467b-8948-4d997e124679",
   "metadata": {},
   "source": [
    "# 11-5-2025 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cf82f-d98b-4ce2-a895-f40a37174fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "num_simulations = 10000  # Adjust as needed\n",
    "\n",
    "# Generate yesterday's date in 'YYYYMMDD' format\n",
    "yesterday = datetime.now() - timedelta(days=5)\n",
    "date_str = yesterday.strftime('%Y%m%d')\n",
    "\n",
    "# Construct filename dynamically\n",
    "filename = f\"MC_day_wise_{num_simulations}_{date_str}.xlsx\"\n",
    "\n",
    "# Define folder path and file path\n",
    "folder_path = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Check and read the file\n",
    "if os.path.exists(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df_MC11 = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if not df_MC11.empty:\n",
    "            print(f\"Reading data from sheet: {sheet_name}\")\n",
    "            #print(df_MC6.head(3))\n",
    "            break\n",
    "    else:\n",
    "        print(\"All sheets are empty in the Excel file.\")\n",
    "else:\n",
    "    print(f\"No file found for date {date_str}: {filename}\")\n",
    "\n",
    "\n",
    "# select particular columns in PG data \n",
    "df1 = df_PG[['Material', 'Purchasing Group Details', 'MRP controller details']]\n",
    "df1.rename(columns={'Material': 'Material Code'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df11 = pd.merge(df_MC11, df1, on='Material Code', how='inner')\n",
    "print(merged_df11.shape)\n",
    "\n",
    "# Create concatenated column\n",
    "merged_df11['Material_PG_Concat'] = merged_df11['Material Code'].astype(str) + \"_\" + merged_df11['Purchasing Group Details'].astype(str)\n",
    "\n",
    "\n",
    "# Ensure the column is numeric (in case it's read as string)\n",
    "merged_df11['Stockout Probability (%)'] = pd.to_numeric(merged_df11['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "# Filter rows with stockout probability >= 50%\n",
    "high_risk_df11 = merged_df11[merged_df11['Stockout Probability (%)'] >= 50]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(high_risk_df11.shape)\n",
    "\n",
    "#print(high_risk_df.head(2))\n",
    "\n",
    "filtered_df11 = high_risk_df11[high_risk_df11['GIT'] == 0]\n",
    "print(filtered_df11.shape)\n",
    "\n",
    "filtered_df11 = filtered_df11[['Running Date','Material Code', 'GIT', 'Purchasing Group Details','Material_PG_Concat']]\n",
    "filtered_df11.head(7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1472b-1421-489a-ae9b-bdb3b5022743",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfgp_filtered_df11 = pd.merge(\n",
    "    filtered_df11,\n",
    "    df_gp[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(merged_dfgp_filtered_df11.shape)\n",
    "merged_dfgp_filtered_df11.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62a761-1bc1-4ad7-9779-ba95621fd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_11may = (\n",
    "    merged_dfgp_filtered_df11.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_counts_11May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_counts_11May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_11may.shape)\n",
    "pg_count_11may.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f68b0a-4abe-47fd-a308-0d863a612bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8dbffca-d13e-4524-bde9-de1f97a3c1d6",
   "metadata": {},
   "source": [
    "# 12-05-2025 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c48e92-b806-4f99-a61a-6ea56b8ba650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#from datetime import datetime, timedelta\n",
    "#import pandas as pd\n",
    "\n",
    "# Set parameters\n",
    "num_simulations = 10000  # Adjust as needed\n",
    "\n",
    "# Generate yesterday's date in 'YYYYMMDD' format\n",
    "yesterday = datetime.now() - timedelta(days=4)\n",
    "date_str = yesterday.strftime('%Y%m%d')\n",
    "\n",
    "# Construct filename dynamically\n",
    "filename = f\"MC_day_wise_{num_simulations}_{date_str}.xlsx\"\n",
    "\n",
    "# Define folder path and file path\n",
    "folder_path = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Check and read the file\n",
    "if os.path.exists(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df_MC12 = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if not df_MC12.empty:\n",
    "            print(f\"Reading data from sheet: {sheet_name}\")\n",
    "            #print(df_MC6.head(3))\n",
    "            break\n",
    "    else:\n",
    "        print(\"All sheets are empty in the Excel file.\")\n",
    "else:\n",
    "    print(f\"No file found for date {date_str}: {filename}\")\n",
    "\n",
    "\n",
    "# select particular columns in PG data \n",
    "df12 = df_PG[['Material', 'Purchasing Group Details', 'MRP controller details']]\n",
    "df12.rename(columns={'Material': 'Material Code'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df12 = pd.merge(df_MC12, df12, on='Material Code', how='inner')\n",
    "print(merged_df12.shape)\n",
    "\n",
    "# Create concatenated column\n",
    "merged_df12['Material_PG_Concat'] = merged_df12['Material Code'].astype(str) + \"_\" + merged_df12['Purchasing Group Details'].astype(str)\n",
    "\n",
    "\n",
    "# Ensure the column is numeric (in case it's read as string)\n",
    "merged_df12['Stockout Probability (%)'] = pd.to_numeric(merged_df12['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "# Filter rows with stockout probability >= 50%\n",
    "high_risk_df12 = merged_df12[merged_df12['Stockout Probability (%)'] >= 50]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(high_risk_df12.shape)\n",
    "\n",
    "#print(high_risk_df.head(2))\n",
    "\n",
    "filtered_df12 = high_risk_df12[high_risk_df12['GIT'] == 0]\n",
    "print(filtered_df12.shape)\n",
    "\n",
    "filtered_df12 = filtered_df12[['Running Date','Material Code', 'GIT', 'Purchasing Group Details','Material_PG_Concat']]\n",
    "filtered_df12.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f935d-bae2-4e1d-9d0d-f7faad7553c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df11_filtered_df12 = pd.merge(\n",
    "    filtered_df12,\n",
    "    merged_dfgp_filtered_df11[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(filtered_df11_filtered_df12.shape)\n",
    "filtered_df11_filtered_df12.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c52c9-edaf-4bf8-866a-c04dcd8b1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_12may = (\n",
    "    filtered_df11_filtered_df12.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_counts_12May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_counts_12May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_12may.shape)\n",
    "pg_count_12may.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2daeacd-d23d-4c54-92f6-fd817bedfb42",
   "metadata": {},
   "source": [
    "# 13-05-2025 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56284c-74c5-4f0a-8c14-0213669340f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#from datetime import datetime, timedelta\n",
    "#import pandas as pd\n",
    "\n",
    "# Set parameters\n",
    "num_simulations = 10000  # Adjust as needed\n",
    "\n",
    "# Generate yesterday's date in 'YYYYMMDD' format\n",
    "yesterday = datetime.now() - timedelta(days=3)\n",
    "date_str = yesterday.strftime('%Y%m%d')\n",
    "\n",
    "# Construct filename dynamically\n",
    "filename = f\"MC_day_wise_{num_simulations}_{date_str}.xlsx\"\n",
    "\n",
    "# Define folder path and file path\n",
    "folder_path = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Check and read the file\n",
    "if os.path.exists(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df_MC13 = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if not df_MC13.empty:\n",
    "            print(f\"Reading data from sheet: {sheet_name}\")\n",
    "            #print(df_MC6.head(3))\n",
    "            break\n",
    "    else:\n",
    "        print(\"All sheets are empty in the Excel file.\")\n",
    "else:\n",
    "    print(f\"No file found for date {date_str}: {filename}\")\n",
    "\n",
    "\n",
    "# select particular columns in PG data \n",
    "df13 = df_PG[['Material', 'Purchasing Group Details', 'MRP controller details']]\n",
    "df13.rename(columns={'Material': 'Material Code'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df13 = pd.merge(df_MC13, df13, on='Material Code', how='inner')\n",
    "print(merged_df13.shape)\n",
    "\n",
    "# Create concatenated column\n",
    "merged_df13['Material_PG_Concat'] = merged_df13['Material Code'].astype(str) + \"_\" + merged_df13['Purchasing Group Details'].astype(str)\n",
    "\n",
    "\n",
    "# Ensure the column is numeric (in case it's read as string)\n",
    "merged_df13['Stockout Probability (%)'] = pd.to_numeric(merged_df13['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "# Filter rows with stockout probability >= 50%\n",
    "high_risk_df13 = merged_df13[merged_df13['Stockout Probability (%)'] >= 50]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(high_risk_df13.shape)\n",
    "\n",
    "#print(high_risk_df.head(2))\n",
    "\n",
    "filtered_df13 = high_risk_df13[high_risk_df13['GIT'] == 0]\n",
    "print(filtered_df13.shape)\n",
    "\n",
    "filtered_df13 = filtered_df13[['Running Date','Material Code', 'GIT', 'Purchasing Group Details','Material_PG_Concat']]\n",
    "filtered_df13.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fa235-034e-4a81-b197-c57dd10c301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df12_filtered_df13 = pd.merge(\n",
    "    filtered_df13,\n",
    "    filtered_df11_filtered_df12[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(filtered_df12_filtered_df13.shape)\n",
    "filtered_df12_filtered_df13.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7360d-2606-4389-a848-be1258b111c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_13may = (\n",
    "    filtered_df12_filtered_df13.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_counts_13May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_counts_13May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_13may.shape)\n",
    "pg_count_13may.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6340f31-8402-4643-a2c4-0df1b2b8d591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbdc6f53-2a54-4d62-9826-974d3dfc7d11",
   "metadata": {},
   "source": [
    "#14.05.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab85aa-357c-4738-9e35-f6a6b45b8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#from datetime import datetime, timedelta\n",
    "#import pandas as pd\n",
    "\n",
    "# Set parameters\n",
    "num_simulations = 10000  # Adjust as needed\n",
    "\n",
    "# Generate yesterday's date in 'YYYYMMDD' format\n",
    "yesterday = datetime.now() - timedelta(days=2)\n",
    "date_str = yesterday.strftime('%Y%m%d')\n",
    "\n",
    "# Construct filename dynamically\n",
    "filename = f\"MC_day_wise_{num_simulations}_{date_str}.xlsx\"\n",
    "\n",
    "# Define folder path and file path\n",
    "folder_path = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Check and read the file\n",
    "if os.path.exists(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df_MC14 = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if not df_MC14.empty:\n",
    "            print(f\"Reading data from sheet: {sheet_name}\")\n",
    "            #print(df_MC6.head(3))\n",
    "            break\n",
    "    else:\n",
    "        print(\"All sheets are empty in the Excel file.\")\n",
    "else:\n",
    "    print(f\"No file found for date {date_str}: {filename}\")\n",
    "\n",
    "\n",
    "# select particular columns in PG data \n",
    "df14 = df_PG[['Material', 'Purchasing Group Details', 'MRP controller details']]\n",
    "df14.rename(columns={'Material': 'Material Code'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df14 = pd.merge(df_MC14, df14, on='Material Code', how='inner')\n",
    "print(merged_df14.shape)\n",
    "\n",
    "# Create concatenated column\n",
    "merged_df14['Material_PG_Concat'] = merged_df14['Material Code'].astype(str) + \"_\" + merged_df14['Purchasing Group Details'].astype(str)\n",
    "\n",
    "\n",
    "# Ensure the column is numeric (in case it's read as string)\n",
    "merged_df14['Stockout Probability (%)'] = pd.to_numeric(merged_df14['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "# Filter rows with stockout probability >= 50%\n",
    "high_risk_df14 = merged_df14[merged_df14['Stockout Probability (%)'] >= 50]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(high_risk_df14.shape)\n",
    "\n",
    "#print(high_risk_df.head(2))\n",
    "\n",
    "filtered_df14 = high_risk_df14[high_risk_df14['GIT'] == 0]\n",
    "print(filtered_df14.shape)\n",
    "\n",
    "filtered_df14 = filtered_df14[['Running Date','Material Code', 'GIT', 'Purchasing Group Details','Material_PG_Concat']]\n",
    "filtered_df14.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ee6d7-4339-4005-a911-54471d134fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df13_filtered_df14 = pd.merge(\n",
    "    filtered_df14,\n",
    "    filtered_df12_filtered_df13[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(filtered_df13_filtered_df14.shape)\n",
    "filtered_df13_filtered_df14.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4ed9c-a6be-4b1a-922e-ff13a8df8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_14may = (\n",
    "    filtered_df13_filtered_df14.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_counts_14May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_counts_14May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_14may.shape)\n",
    "pg_count_14may.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50d4a6-d1ee-43ab-b2dc-7ecd01a7b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_11_12 = pd.merge(pg_count_11may, pg_count_12may, on='Purchasing Group Details', how='outer')\n",
    "merged_12_13 = pd.merge(merged_11_12, pg_count_13may, on='Purchasing Group Details', how='outer')\n",
    "merged_13_14 = pd.merge(merged_12_13, pg_count_14may, on='Purchasing Group Details', how='outer')\n",
    "#merged_14_15 = pd.merge(merged_13_14, pg_count_15may, on='Purchasing Group Details', how='outer')\n",
    "\n",
    "\n",
    "# 5. Fill NaN values with 0 and convert to int\n",
    "merged_13_14[['Buyer_counts_11May', 'Buyer_counts_12May','Buyer_counts_13May','Buyer_counts_14May']] = (\n",
    "    merged_13_14[['Buyer_counts_11May', 'Buyer_counts_12May','Buyer_counts_13May','Buyer_counts_14May']].fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# 6. Optional display\n",
    "print(merged_13_14.shape)\n",
    "#merged_13_14.head(10)\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Example column names: 'Buyer_counts_11May', 'Buyer_counts_15May'\n",
    "\n",
    "merged_13_14['pending %'] = (merged_13_14['Buyer_counts_14May'] / merged_13_14['Buyer_counts_11May']) * 100\n",
    "merged_13_14['Efficiency %'] = 100 - merged_13_14['pending %']\n",
    "\n",
    "# Optional: Round the values for better readability\n",
    "merged_13_14['pending %'] = merged_13_14['pending %'].round(2).astype(str) + '%'\n",
    "merged_13_14['Efficiency %'] = merged_13_14['Efficiency %'].round(2).astype(str) + '%'\n",
    "\n",
    "# Display result\n",
    "merged_13_14.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fb112-6b1f-46e6-969d-a9e7cc7b4d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd664b9-80b5-4d65-812a-43399fce2d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a511f9-a1ba-4f0a-8a11-a35babde2cb4",
   "metadata": {},
   "source": [
    "# Week -02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025d47a-3172-4005-8d3b-2b7c1665b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#from datetime import datetime, timedelta\n",
    "#import pandas as pd\n",
    "\n",
    "# Set parameters\n",
    "num_simulations = 10000  # Adjust as needed\n",
    "\n",
    "# Generate yesterday's date in 'YYYYMMDD' format\n",
    "yesterday = datetime.now() - timedelta(days=4)\n",
    "date_str = yesterday.strftime('%Y%m%d')\n",
    "\n",
    "# Construct filename dynamically\n",
    "filename = f\"MC_day_wise_{num_simulations}_{date_str}.xlsx\"\n",
    "\n",
    "# Define folder path and file path\n",
    "folder_path = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Check and read the file\n",
    "if os.path.exists(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df_MC112 = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if not df_MC112.empty:\n",
    "            print(f\"Reading data from sheet: {sheet_name}\")\n",
    "            #print(df_MC6.head(3))\n",
    "            break\n",
    "    else:\n",
    "        print(\"All sheets are empty in the Excel file.\")\n",
    "else:\n",
    "    print(f\"No file found for date {date_str}: {filename}\")\n",
    "\n",
    "\n",
    "# select particular columns in PG data \n",
    "df112 = df_PG[['Material', 'Purchasing Group Details', 'MRP controller details']]\n",
    "df112.rename(columns={'Material': 'Material Code'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df112 = pd.merge(df_MC112, df112, on='Material Code', how='inner')\n",
    "print(merged_df112.shape)\n",
    "\n",
    "# Create concatenated column\n",
    "merged_df112['Material_PG_Concat'] = merged_df112['Material Code'].astype(str) + \"_\" + merged_df112['Purchasing Group Details'].astype(str)\n",
    "\n",
    "\n",
    "# Ensure the column is numeric (in case it's read as string)\n",
    "merged_df112['Stockout Probability (%)'] = pd.to_numeric(merged_df112['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "# Filter rows with stockout probability >= 50%\n",
    "high_risk_df112 = merged_df112[merged_df112['Stockout Probability (%)'] >= 50]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(high_risk_df112.shape)\n",
    "\n",
    "#print(high_risk_df.head(2))\n",
    "\n",
    "filtered_df112 = high_risk_df112[high_risk_df112['GIT'] == 0]\n",
    "print(filtered_df112.shape)\n",
    "\n",
    "filtered_df112 = filtered_df112[['Running Date','Material Code', 'GIT', 'Purchasing Group Details','Material_PG_Concat']]\n",
    "filtered_df112.head(7)\n",
    "\n",
    "merged_dfgp_filtered_df112 = pd.merge(\n",
    "    filtered_df112,\n",
    "    df_gp[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(merged_dfgp_filtered_df112.shape)\n",
    "merged_dfgp_filtered_df112.head(7)\n",
    "\n",
    "pg_count_112 = (\n",
    "    merged_dfgp_filtered_df112.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_12May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_12May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_112.shape)\n",
    "pg_count_112.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94f65-8194-40ca-a0ba-8c26ac4f65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df12_filtered_df13_new = pd.merge(\n",
    "    filtered_df13,\n",
    "    merged_dfgp_filtered_df112[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(filtered_df12_filtered_df13_new.shape)\n",
    "filtered_df12_filtered_df13_new.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bcf07-53c8-42b5-aa9b-1b35a7250f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_13 = (\n",
    "    filtered_df12_filtered_df13_new.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_13May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_13May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_13.shape)\n",
    "pg_count_13.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde412c9-6e7f-4d1e-b859-d0121faf8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df13_filtered_df14_new = pd.merge(\n",
    "    filtered_df14,\n",
    "    filtered_df12_filtered_df13_new[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(filtered_df13_filtered_df14_new.shape)\n",
    "filtered_df13_filtered_df14_new.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4153518-f579-427c-a93f-77667aee997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_14 = (\n",
    "    filtered_df13_filtered_df14_new.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_14May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_14May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_14.shape)\n",
    "pg_count_14.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8730d72-2bc6-4584-af9e-67d5fe0c9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#from datetime import datetime, timedelta\n",
    "#import pandas as pd\n",
    "\n",
    "# Set parameters\n",
    "num_simulations = 10000  # Adjust as needed\n",
    "\n",
    "# Generate yesterday's date in 'YYYYMMDD' format\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "date_str = yesterday.strftime('%Y%m%d')\n",
    "\n",
    "# Construct filename dynamically\n",
    "filename = f\"MC_day_wise_{num_simulations}_{date_str}.xlsx\"\n",
    "\n",
    "# Define folder path and file path\n",
    "folder_path = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "# Check and read the file\n",
    "if os.path.exists(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df_MC15 = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if not df_MC15.empty:\n",
    "            print(f\"Reading data from sheet: {sheet_name}\")\n",
    "            #print(df_MC15.head(3))\n",
    "            break\n",
    "    else:\n",
    "        print(\"All sheets are empty in the Excel file.\")\n",
    "else:\n",
    "    print(f\"No file found for date {date_str}: {filename}\")\n",
    "\n",
    "\n",
    "# select particular columns in PG data \n",
    "df15 = df_PG[['Material', 'Purchasing Group Details', 'MRP controller details']]\n",
    "df15.rename(columns={'Material': 'Material Code'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df15 = pd.merge(df_MC15, df15, on='Material Code', how='inner')\n",
    "print(merged_df15.shape)\n",
    "\n",
    "# Create concatenated column\n",
    "merged_df15['Material_PG_Concat'] = merged_df15['Material Code'].astype(str) + \"_\" + merged_df15['Purchasing Group Details'].astype(str)\n",
    "\n",
    "\n",
    "# Ensure the column is numeric (in case it's read as string)\n",
    "merged_df15['Stockout Probability (%)'] = pd.to_numeric(merged_df15['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "# Filter rows with stockout probability >= 50%\n",
    "high_risk_df15 = merged_df15[merged_df15['Stockout Probability (%)'] >= 50]\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "print(high_risk_df15.shape)\n",
    "\n",
    "#print(high_risk_df.head(2))\n",
    "\n",
    "filtered_df15 = high_risk_df15[high_risk_df15['GIT'] == 0]\n",
    "print(filtered_df15.shape)\n",
    "\n",
    "filtered_df15 = filtered_df15[['Running Date','Material Code', 'GIT', 'Purchasing Group Details','Material_PG_Concat']]\n",
    "filtered_df15.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fcb16-b96c-4d0d-8a2f-6b57a6c835f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df14_filtered_df15_new = pd.merge(\n",
    "    filtered_df15,\n",
    "    filtered_df13_filtered_df14_new[['Material_PG_Concat']],  # Only use the join column from df_gp\n",
    "    on='Material_PG_Concat',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(filtered_df14_filtered_df15_new.shape)\n",
    "filtered_df14_filtered_df15_new.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1cdbb-cd4e-4a47-8fa1-a81a81db10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_count_15 = (\n",
    "    filtered_df14_filtered_df15_new.groupby('Purchasing Group Details')\n",
    "    .size()\n",
    "    .reset_index(name='Buyer_15May')\n",
    "    .sort_values(by=['Purchasing Group Details', 'Buyer_15May'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "print(pg_count_15.shape)\n",
    "pg_count_15.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69398b-f7a2-49c2-9a49-9ccc604f5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.merge(pg_count_112, pg_count_13, on='Purchasing Group Details', how='outer')\n",
    "merged2 = pd.merge(merged1, pg_count_14, on='Purchasing Group Details', how='outer')\n",
    "merged3 = pd.merge(merged2, pg_count_15, on='Purchasing Group Details', how='outer')\n",
    "#merged_14_15 = pd.merge(merged_13_14, pg_count_15may, on='Purchasing Group Details', how='outer')\n",
    "\n",
    "\n",
    "# 5. Fill NaN values with 0 and convert to int\n",
    "merged3[['Buyer_12May', 'Buyer_13May','Buyer_14May','Buyer_15May']] = (\n",
    "    merged3[['Buyer_12May', 'Buyer_13May','Buyer_14May','Buyer_15May']].fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# 6. Optional display\n",
    "print(merged3.shape)\n",
    "#merged_13_14.head(10)\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Example column names: 'Buyer_counts_11May', 'Buyer_counts_15May'\n",
    "\n",
    "merged3['pending %'] = (merged3['Buyer_15May'] / merged3['Buyer_12May']) * 100\n",
    "merged3['Efficiency %'] = 100 - merged3['pending %']\n",
    "\n",
    "# Optional: Round the values for better readability\n",
    "merged3['pending %'] = merged3['pending %'].round(2).astype(str) + '%'\n",
    "merged3['Efficiency %'] = merged3['Efficiency %'].round(2).astype(str) + '%'\n",
    "\n",
    "# Display result\n",
    "merged3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b32aab-58c1-4487-afe8-ecf90b1555a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fa76c-e34e-48a5-974e-8fcfa46e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Merge on 'Purchasing Group Details' column\n",
    "combined_df = pd.merge(merged_13_14, merged3, on='Purchasing Group Details', suffixes=('_11to14', '_12to15'))\n",
    "print(combined_df.shape)\n",
    "# Show result\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2137ee8-bb21-4fb7-9697-ed6ac7f3db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_excel('BLACK_FORECATING_DASHBOARD_OUTPUt.xlsx', index=False)\n",
    "\n",
    "print(\"Excel file 'BLACK_FORECATING_DASHBOARD_OUTPUt.xlsx' has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abe2da-77d3-4076-bc52-2a2491b98594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa22cf8-bd78-4ca7-aaf8-904447dae460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdf305-ad07-474f-9602-ff0c2c5f2790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db99c54-bbaf-4cc2-ad7f-1481636e246c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103fe22-e22e-4be9-b846-a0c5e9f92221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d82f17c-8905-407d-be47-9a345f3c4629",
   "metadata": {},
   "source": [
    "#### code for loop creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dbdc8ba-4755-4e32-bac0-7867cef4b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Purchasing Group Details  Buyer_counts_26May  Buyer_counts_27May  \\\n",
      "0               Akshay Raj                  35                  27   \n",
      "1           Amarjeet Yadav                  37                  28   \n",
      "\n",
      "   Buyer_counts_28May  Buyer_counts_29May pending % Efficiency %  \n",
      "0                  20                  12    34.29%       65.71%  \n",
      "1                  15                  13    35.14%       64.86%  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "# === Constants ===\n",
    "NUM_SIMULATIONS = 10000\n",
    "INPUT_FOLDER = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Input\"\n",
    "OUTPUT_FOLDER = \"/home/VECVNET/zzasom/jupyter/01_Development_work/Black_Forecasting/Deployment/Output\"\n",
    "BASE_DATE = datetime(2025, 5, 26)  # Fixed base date: 12 May 2025\n",
    "DAYS_AHEAD = 3  # Today + next 3 days\n",
    "\n",
    "# === Load PG Mapping File ===\n",
    "pg_file_path = os.path.join(INPUT_FOLDER, \"MRPcontrollerPurchasingGrp.csv\")\n",
    "if not os.path.exists(pg_file_path):\n",
    "    raise FileNotFoundError(\"PG file not found.\")\n",
    "\n",
    "df_pg = pd.read_csv(pg_file_path)\n",
    "df_pg['Material_PG_Concat'] = df_pg['Material'].astype(str) + \"_\" + df_pg['Purchasing Group Details'].astype(str)\n",
    "df_pg_merge = df_pg[['Material', 'Purchasing Group Details', 'MRP controller details']].rename(\n",
    "    columns={'Material': 'Material Code'}\n",
    ")\n",
    "\n",
    "# === Function to Process a Single Date ===\n",
    "def process_day(date_obj, filter_from_df):\n",
    "    date_str = date_obj.strftime('%Y%m%d')\n",
    "    file_name = f\"MC_day_wise_{NUM_SIMULATIONS}_{date_str}.xlsx\"\n",
    "    file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[{date_str}] File not found: {file_name}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df_mc = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            if not df_mc.empty:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"[{date_str}] All sheets are empty.\")\n",
    "            return None, None\n",
    "\n",
    "        # Merge PG info\n",
    "        df_mc = pd.merge(df_mc, df_pg_merge, on='Material Code', how='inner')\n",
    "        df_mc['Material_PG_Concat'] = df_mc['Material Code'].astype(str) + \"_\" + df_mc['Purchasing Group Details'].astype(str)\n",
    "        df_mc['Stockout Probability (%)'] = pd.to_numeric(df_mc['Stockout Probability (%)'], errors='coerce')\n",
    "\n",
    "        # Filter\n",
    "        high_risk_df = df_mc[(df_mc['Stockout Probability (%)'] >= 50) & (df_mc['GIT'] == 0)]\n",
    "        filtered_df = high_risk_df[['Running Date', 'Material Code', 'GIT', 'Purchasing Group Details', 'Material_PG_Concat']]\n",
    "\n",
    "        if filter_from_df is not None:\n",
    "            filtered_df = filtered_df[filtered_df['Material_PG_Concat'].isin(filter_from_df['Material_PG_Concat'])]\n",
    "\n",
    "        # Count high-risk parts per PG\n",
    "        col_label = f\"Buyer_counts_{date_obj.strftime('%d%b')}\"\n",
    "        pg_counts = (\n",
    "            filtered_df.groupby('Purchasing Group Details')\n",
    "            .size()\n",
    "            .reset_index(name=col_label)\n",
    "        )\n",
    "\n",
    "        return filtered_df, pg_counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{date_str}] Error processing: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# === Process Multiple Days ===\n",
    "filtered_df_acc = None\n",
    "pg_counts_all = []\n",
    "\n",
    "for i in range(DAYS_AHEAD + 1):  # today to today + 3\n",
    "    run_date = BASE_DATE + timedelta(days=i)\n",
    "    filtered_df_acc, pg_count = process_day(run_date, filtered_df_acc)\n",
    "    if pg_count is not None:\n",
    "        pg_counts_all.append(pg_count)\n",
    "\n",
    "# === Merge and Compute Efficiency ===\n",
    "if pg_counts_all:\n",
    "    merged_pg_counts = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on='Purchasing Group Details', how='outer'),\n",
    "        pg_counts_all\n",
    "    )\n",
    "\n",
    "    # Fill NaN with 0 and convert to int\n",
    "    count_cols = [col for col in merged_pg_counts.columns if col.startswith(\"Buyer_counts_\")]\n",
    "    merged_pg_counts[count_cols] = merged_pg_counts[count_cols].fillna(0).astype(int)\n",
    "\n",
    "    if len(count_cols) >= 2:\n",
    "        first_col, last_col = count_cols[0], count_cols[-1]\n",
    "        merged_pg_counts['pending %'] = (merged_pg_counts[last_col] / merged_pg_counts[first_col].replace(0, 1)) * 100\n",
    "        merged_pg_counts['Efficiency %'] = 100 - merged_pg_counts['pending %']\n",
    "\n",
    "        merged_pg_counts['pending %'] = merged_pg_counts['pending %'].round(2).astype(str) + '%'\n",
    "        merged_pg_counts['Efficiency %'] = merged_pg_counts['Efficiency %'].round(2).astype(str) + '%'\n",
    "\n",
    "    print(merged_pg_counts.head(2))\n",
    "else:\n",
    "    print(\"No data processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63df0587-2cc7-45a7-812b-9956108a3027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Purchasing Group Details</th>\n",
       "      <th>Buyer_counts_26May</th>\n",
       "      <th>Buyer_counts_27May</th>\n",
       "      <th>Buyer_counts_28May</th>\n",
       "      <th>Buyer_counts_29May</th>\n",
       "      <th>pending %</th>\n",
       "      <th>Efficiency %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akshay Raj</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>34.29%</td>\n",
       "      <td>65.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amarjeet Yadav</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>35.14%</td>\n",
       "      <td>64.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arun Kumar Shah</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>28.95%</td>\n",
       "      <td>71.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashish Purohit</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>58.33%</td>\n",
       "      <td>41.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deepak Deshpande</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>81.25%</td>\n",
       "      <td>18.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deepak Raipure</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>27.08%</td>\n",
       "      <td>72.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Girish R Sonawane</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>30.77%</td>\n",
       "      <td>69.23%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Purchasing Group Details  Buyer_counts_26May  Buyer_counts_27May  \\\n",
       "0               Akshay Raj                  35                  27   \n",
       "1           Amarjeet Yadav                  37                  28   \n",
       "2          Arun Kumar Shah                  38                  24   \n",
       "3           Ashish Purohit                  12                   9   \n",
       "4         Deepak Deshpande                  32                  29   \n",
       "5           Deepak Raipure                  48                  32   \n",
       "6        Girish R Sonawane                  52                  31   \n",
       "\n",
       "   Buyer_counts_28May  Buyer_counts_29May pending % Efficiency %  \n",
       "0                  20                  12    34.29%       65.71%  \n",
       "1                  15                  13    35.14%       64.86%  \n",
       "2                  15                  11    28.95%       71.05%  \n",
       "3                   7                   7    58.33%       41.67%  \n",
       "4                  28                  26    81.25%       18.75%  \n",
       "5                  21                  13    27.08%       72.92%  \n",
       "6                  21                  16    30.77%       69.23%  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_pg_counts.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5da779b-420d-47f2-b461-64f399af7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pg_counts.to_excel('BF_26May_Dashboard_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f59fbd0-f567-4222-96fb-ea55228d6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(merged_pg_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da57627-4ca9-4379-9844-d56eb4f932ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write PySpark DataFrame to the database\n",
    "spark_df.write.mode(\"overwrite\").format(\"jdbc\").options(\n",
    "    driver=driver,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    dbtable=\"Black_Forecasting_Dashboard\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final output store in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c868d2-8a32-4cf2-9ef8-ac0dfc5acfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06391301-25bc-4305-b2c1-16fb9aa35203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
